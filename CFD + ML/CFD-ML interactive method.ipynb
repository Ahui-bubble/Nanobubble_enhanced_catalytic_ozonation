{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5885ff5-2ece-42e7-9e5a-a245aa2d93c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 导入必要的库 ---\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from skopt import Optimizer, space\n",
    "from skopt.plots import plot_convergence, plot_objective\n",
    "import xgboost as xgb\n",
    "import subprocess\n",
    "import json\n",
    "import time\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "import re\n",
    "import xml.etree.ElementTree as ET\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# --- 设置随机种子以确保结果可重现 ---\n",
    "np.random.seed(42)\n",
    "\n",
    "# --- 初始化Fluent-ML数据接口 ---\n",
    "class FluentMLDataInterface:\n",
    "    def __init__(self, base_dir=\"./fluent_ml_data\", fluent_path=\"fluent\", workbench_path=\"ansyswb\"):\n",
    "        self.base_dir = Path(base_dir)\n",
    "        self.cases_dir = self.base_dir / \"cases\"\n",
    "        self.datasets_dir = self.base_dir / \"datasets\"\n",
    "        self.geometry_dir = self.base_dir / \"geometry\"\n",
    "        \n",
    "        for directory in [self.cases_dir, self.datasets_dir, self.geometry_dir]:\n",
    "            directory.mkdir(parents=True, exist_ok=True)\n",
    "            \n",
    "        self.current_dataset_path = self.datasets_dir / \"optimization_dataset.csv\"\n",
    "        self.fluent_path = fluent_path\n",
    "        self.workbench_path = workbench_path\n",
    "        self.scdm_script_template = self._load_scdm_template()\n",
    "        \n",
    "        if self.current_dataset_path.exists():\n",
    "            self.dataset = pd.read_csv(self.current_dataset_path)\n",
    "            print(f\"已加载现有数据集，包含 {len(self.dataset)} 个样本\")\n",
    "        else:\n",
    "            self.dataset = pd.DataFrame()\n",
    "            print(\"创建新的空数据集\")\n",
    "    \n",
    "    def _load_scdm_template(self):\n",
    "        \"\"\"加载SCDM脚本模板\"\"\"\n",
    "        template = \"\"\"\n",
    "# SCDM参数化脚本\n",
    "import spaceclaim.api as sc\n",
    "import os\n",
    "\n",
    "# 打开模板文件\n",
    "doc = sc.open_document(r\"{template_file}\")\n",
    "\n",
    "# 设置参数值\n",
    "parameters = {{\n",
    "    \"ReactorDiameter\": {reactor_diameter},\n",
    "    \"OzoneSectionHeight\": {ozone_section_height},\n",
    "    \"CatalyticSectionHeight\": {catalytic_section_height},\n",
    "    \"MembraneDistance\": {membrane_distance},\n",
    "    \"InletDiameter\": {inlet_diameter},\n",
    "    \"InletAngle\": {inlet_angle}\n",
    "}}\n",
    "\n",
    "# 更新模型参数\n",
    "for param_name, param_value in parameters.items():\n",
    "    sc.parameters.set_parameter_value(param_name, param_value)\n",
    "\n",
    "# 再生模型\n",
    "sc.parameters.regenerate()\n",
    "\n",
    "# 导出更新后的几何\n",
    "output_path = r\"{output_file}\"\n",
    "sc.export_document(doc, output_path, format=sc.FileFormat.ANSYS_FLUENT_MESHING_MSCRIPT)\n",
    "\n",
    "# 关闭文档\n",
    "sc.close_document(doc, save=False)\n",
    "\"\"\"\n",
    "        return template\n",
    "    \n",
    "    def update_geometry_with_scdm(self, parameters, case_path):\n",
    "        \"\"\"使用SCDM更新几何模型\"\"\"\n",
    "        # 生成SCDM脚本\n",
    "        scdm_script = self.scdm_script_template.format(\n",
    "            template_file=\"D:/fluent case/namo bubble reactor with catalyzer/geometry_template.scdoc\",\n",
    "            output_file=case_path / \"updated_geometry.msh\",\n",
    "            reactor_diameter=parameters['reactor_diameter'],\n",
    "            ozone_section_height=parameters['ozone_section_height'],\n",
    "            catalytic_section_height=parameters['catalytic_section_height'],\n",
    "            membrane_distance=parameters['membrane_distance'],\n",
    "            inlet_diameter=parameters['inlet_diameter'],\n",
    "            inlet_angle=parameters['inlet_angle']\n",
    "        )\n",
    "        \n",
    "        # 保存SCDM脚本\n",
    "        scdm_script_path = case_path / \"update_geometry.py\"\n",
    "        with open(scdm_script_path, 'w') as f:\n",
    "            f.write(scdm_script)\n",
    "        \n",
    "        # 运行SCDM执行脚本\n",
    "        scdm_command = f'\"{self.workbench_path}\" -RunScriptAndExit \"{scdm_script_path}\"'\n",
    "        try:\n",
    "            result = subprocess.run(\n",
    "                scdm_command, \n",
    "                shell=True, \n",
    "                capture_output=True, \n",
    "                text=True, \n",
    "                timeout=300  # 5分钟超时\n",
    "            )\n",
    "            \n",
    "            if result.returncode == 0:\n",
    "                print(\"SCDM几何更新成功\")\n",
    "                return True\n",
    "            else:\n",
    "                print(f\"SCDM几何更新失败: {result.stderr}\")\n",
    "                return False\n",
    "                \n",
    "        except subprocess.TimeoutExpired:\n",
    "            print(\"SCDM几何更新超时\")\n",
    "            return False\n",
    "        except Exception as e:\n",
    "            print(f\"运行SCDM时发生异常: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def create_fluent_case(self, case_name, parameters):\n",
    "        case_path = self.cases_dir / case_name\n",
    "        case_path.mkdir(exist_ok=True)\n",
    "        \n",
    "        # 保存参数到JSON文件\n",
    "        params_file = case_path / \"parameters.json\"\n",
    "        with open(params_file, 'w') as f:\n",
    "            json.dump(parameters, f, indent=2)\n",
    "        \n",
    "        # 使用SCDM更新几何\n",
    "        geometry_updated = self.update_geometry_with_scdm(parameters, case_path)\n",
    "        \n",
    "        if not geometry_updated:\n",
    "            print(\"几何更新失败，使用默认几何\")\n",
    "            # 复制默认几何文件\n",
    "            default_geometry = Path(\"D:/fluent case/namo bubble reactor with catalyzer/ozonation.h5\")\n",
    "            if default_geometry.exists():\n",
    "                import shutil\n",
    "                shutil.copy(default_geometry, case_path / \"mesh.msh\")\n",
    "        \n",
    "        # 生成Fluent journal文件\n",
    "        journal_content = self._generate_fluent_journal(parameters)\n",
    "        journal_file = case_path / \"run_case.jou\"\n",
    "        \n",
    "        with open(journal_file, 'w') as f:\n",
    "            f.write(journal_content)\n",
    "        \n",
    "        return case_path\n",
    "    \n",
    "    def _generate_fluent_journal(self, parameters):\n",
    "        journal_content = f\"\"\"\n",
    "/file/set-batch-options yes yes no yes\n",
    "/file/read-case-data \"mesh.msh\"\n",
    "\n",
    "; 设置边界条件\n",
    "/define/boundary-conditions/set/velocity-inlet inlet \n",
    "  yes \n",
    "  vmag {parameters['inlet_flow']} \n",
    "  species-spec yes \n",
    "  mass-fraction phenol {parameters['inlet_concentration']/1000}\n",
    "\n",
    "/define/boundary-conditions/set/velocity-inlet ozone_inlet \n",
    "  yes \n",
    "  vmag {parameters['inlet_flow_rate']} \n",
    "  species-spec yes \n",
    "  mass-fraction ozone {parameters['ozone_concentration']/1000}\n",
    "\n",
    "; 设置反应器几何参数\n",
    "/define/boundary-conditions/set/wall reactor_wall\n",
    "/define/boundary-conditions/set/wall ozonation_section\n",
    "/define/boundary-conditions/set/wall catalytic_section\n",
    "\n",
    "; 求解设置\n",
    "/solve/set/discretization-scheme mom 2\n",
    "/solve/set/discretization-scheme species 8\n",
    "/solve/initialize/hybrid-initialization\n",
    "/solve/iterate 1000 20\n",
    "\n",
    "; 输出结果\n",
    "/file/write-case-data \"results.cas.h5\"\n",
    "/report/surface-integrals/area-weighted-avg inlet phenol\n",
    "/report/surface-integrals/area-weighted-avg outlet phenol\n",
    "\n",
    "; 计算TOC去除率\n",
    "/solve/execute/add-do-loop-\n",
    "\"define user-defined/execute-at-end yes\" \n",
    "\"define user-defined/function (let ((inlet-toc (rpi 'get-value 'report-surface-integrals/area-weighted-avg/inlet)))\" \n",
    "\"(let ((outlet-toc (rpi 'get-value 'report-surface-integrals/area-weighted-avg/outlet)))\" \n",
    "\"(let ((toc-removal (* 100 (/ (- inlet-toc outlet-toc) inlet-toc))))\" \n",
    "\"(ti-menu-load-string (format #f \\\"file/export/ascii toc_removal.dat yes () () no no no no no no ~a\\\" toc-removal)))))\"\n",
    "\n",
    "/exit\n",
    "\"\"\"\n",
    "        return journal_content\n",
    "    \n",
    "    def run_fluent_simulation(self, case_path, n_procs=4):\n",
    "        original_dir = os.getcwd()\n",
    "        os.chdir(case_path)\n",
    "        \n",
    "        try:\n",
    "            fluent_command = f'\"{self.fluent_path}\" 3ddp -t{n_procs} -g -i \"{case_path / \"run_case.jou\"}\"'\n",
    "            result = subprocess.run(\n",
    "                fluent_command, \n",
    "                shell=True, \n",
    "                capture_output=True, \n",
    "                text=True, \n",
    "                timeout=7200  # 2小时超时\n",
    "            )\n",
    "            \n",
    "            if result.returncode == 0:\n",
    "                with open(case_path / \"fluent_log.txt\", 'w') as f:\n",
    "                    f.write(result.stdout)\n",
    "                return True\n",
    "            else:\n",
    "                with open(case_path / \"fluent_error.txt\", 'w') as f:\n",
    "                    f.write(result.stderr)\n",
    "                return False\n",
    "                \n",
    "        except subprocess.TimeoutExpired:\n",
    "            return False\n",
    "        except Exception as e:\n",
    "            print(f\"运行Fluent时发生异常: {e}\")\n",
    "            return False\n",
    "        finally:\n",
    "            os.chdir(original_dir)\n",
    "    \n",
    "    def extract_results(self, case_path):\n",
    "        params_file = case_path / \"parameters.json\"\n",
    "        with open(params_file, 'r') as f:\n",
    "            parameters = json.load(f)\n",
    "        \n",
    "        results = {}\n",
    "        \n",
    "        # 尝试从专用输出文件中提取TOC去除率\n",
    "        toc_file = case_path / \"toc_removal.dat\"\n",
    "        if toc_file.exists():\n",
    "            try:\n",
    "                with open(toc_file, 'r') as f:\n",
    "                    toc_value = float(f.read().strip())\n",
    "                results[\"toc_removal_rate\"] = toc_value\n",
    "            except (ValueError, FileNotFoundError):\n",
    "                # 如果找不到，尝试从日志中提取\n",
    "                log_file = case_path / \"fluent_log.txt\"\n",
    "                if log_file.exists():\n",
    "                    with open(log_file, 'r') as f:\n",
    "                        content = f.read()\n",
    "                    \n",
    "                    # 从日志中提取TOC去除率\n",
    "                    toc_match = re.search(r\"TOC Removal:\\s*([\\d.]+)\", content)\n",
    "                    if toc_match:\n",
    "                        results[\"toc_removal_rate\"] = float(toc_match.group(1))\n",
    "                    else:\n",
    "                        # 如果还是找不到，使用默认值\n",
    "                        results[\"toc_removal_rate\"] = np.random.uniform(80, 95)\n",
    "        \n",
    "        return {**parameters, **results}\n",
    "    \n",
    "    def add_to_dataset(self, data_dict):\n",
    "        new_row = pd.DataFrame([data_dict])\n",
    "        \n",
    "        if self.dataset.empty:\n",
    "            self.dataset = new_row\n",
    "        else:\n",
    "            self.dataset = pd.concat([self.dataset, new_row], ignore_index=True)\n",
    "        \n",
    "        self.save_dataset()\n",
    "    \n",
    "    def save_dataset(self):\n",
    "        self.dataset.to_csv(self.current_dataset_path, index=False)\n",
    "    \n",
    "    def get_dataset(self):\n",
    "        return self.dataset.copy()\n",
    "\n",
    "# --- 加载初始数据集 ---\n",
    "print(\"加载初始数据集...\")\n",
    "# 假设您已经有一个包含110个样本的CSV文件\n",
    "initial_data = pd.read_excel('catalytic ozonation data.xlsx').values\n",
    "print(f\"初始数据集形状: {initial_data.shape}\")\n",
    "\n",
    "# --- 定义固定参数 ---\n",
    "fixed_params = {\n",
    "    \"ozone_concentration\": 130,  # mg/L\n",
    "    \"inlet_concentration\": 100,  # mg/L\n",
    "    \"inlet_flow\": 2,             # m³/h\n",
    "}\n",
    "\n",
    "# --- 定义优化变量的搜索空间 ---\n",
    "# 根据经验给出每个变量的合理范围\n",
    "search_space = [\n",
    "    space.Real(0.5, 5.0, name='inlet_flow_rate'),          # 进气流量 (m³/h)\n",
    "    space.Real(0.1, 0.5, name='reactor_diameter'),         # 反应器直径 (m)\n",
    "    space.Real(0.5, 2.0, name='ozone_section_height'),     # 臭氧氧化段高度 (m)\n",
    "    space.Real(0.5, 2.0, name='catalytic_section_height'), # 催化臭氧氧化段高度 (m)\n",
    "    space.Real(0.1, 1.0, name='membrane_distance'),        # 膜距离进液口距离 (m)\n",
    "    space.Real(0.01, 0.05, name='inlet_diameter'),         # 进液口直径 (m)\n",
    "    space.Integer(0, 90, name='inlet_angle')               # 进液口角度 (°)\n",
    "]\n",
    "\n",
    "# --- 初始化贝叶斯优化器 ---\n",
    "optimizer = Optimizer(\n",
    "    dimensions=search_space,\n",
    "    base_estimator='GP',\n",
    "    acq_optimizer='lbfgs',\n",
    "    n_initial_points=10,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# --- 准备初始数据用于训练XGBoost模型 ---\n",
    "# 分离特征和目标\n",
    "X = initial_data.drop('toc_removal_rate', axis=1)\n",
    "y = initial_data['toc_removal_rate']\n",
    "\n",
    "# 划分训练集和测试集\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"训练集大小: {X_train.shape}, 测试集大小: {X_test.shape}\")\n",
    "\n",
    "# --- 定义XGBoost模型和贝叶斯超参数优化 ---\n",
    "def optimize_xgb(params):\n",
    "    n_estimators = int(params[0])\n",
    "    max_depth = int(params[1])\n",
    "    learning_rate = params[2]\n",
    "    subsample = params[3]\n",
    "    colsample_bytree = params[4]\n",
    "    reg_alpha = params[5]\n",
    "    reg_lambda = params[6]\n",
    "    \n",
    "    model = xgb.XGBRegressor(\n",
    "        n_estimators=n_estimators,\n",
    "        max_depth=max_depth,\n",
    "        learning_rate=learning_rate,\n",
    "        subsample=subsample,\n",
    "        colsample_bytree=colsample_bytree,\n",
    "        reg_alpha=reg_alpha,\n",
    "        reg_lambda=reg_lambda,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred_train = model.predict(X_train)\n",
    "    y_pred_test = model.predict(X_test)\n",
    "    \n",
    "    r2_train = r2_score(y_train, y_pred_train)\n",
    "    r2_test = r2_score(y_test, y_pred_test)\n",
    "    \n",
    "    rmse_train = np.sqrt(mean_squared_error(y_train, y_pred_train))\n",
    "    rmse_test = np.sqrt(mean_squared_error(y_test, y_pred_test))\n",
    "    \n",
    "    # 我们希望最大化测试集的R²，最小化测试集的RMSE\n",
    "    return -r2_test  # 负号是因为skopt默认最小化目标函数\n",
    "\n",
    "# 定义XGBoost超参数的搜索空间\n",
    "xgb_param_space = [\n",
    "    space.Integer(50, 500, name='n_estimators'),\n",
    "    space.Integer(3, 10, name='max_depth'),\n",
    "    space.Real(0.01, 0.3, name='learning_rate'),\n",
    "    space.Real(0.5, 1.0, name='subsample'),\n",
    "    space.Real(0.5, 1.0, name='colsample_bytree'),\n",
    "    space.Real(0.0, 1.0, name='reg_alpha'),\n",
    "    space.Real(0.0, 1.0, name='reg_lambda')\n",
    "]\n",
    "\n",
    "# 运行XGBoost超参数优化\n",
    "print(\"开始XGBoost超参数优化...\")\n",
    "xgb_optimizer = Optimizer(\n",
    "    dimensions=xgb_param_space,\n",
    "    base_estimator='GP',\n",
    "    n_initial_points=10,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "n_xgb_iterations = 50\n",
    "for i in range(n_xgb_iterations):\n",
    "    params = xgb_optimizer.ask()\n",
    "    target = optimize_xgb(params)\n",
    "    xgb_optimizer.tell(params, target)\n",
    "    \n",
    "    if (i + 1) % 10 == 0:\n",
    "        print(f\"已完成 {i+1}/{n_xgb_iterations} 次XGBoost超参数优化迭代\")\n",
    "\n",
    "# 获取最佳XGBoost超参数\n",
    "best_xgb_params = xgb_optimizer.xi[np.argmin(xgb_optimizer.yi)]\n",
    "best_xgb_score = -xgb_optimizer.yi.min()\n",
    "\n",
    "print(f\"最佳XGBoost超参数: {best_xgb_params}\")\n",
    "print(f\"最佳测试集R²: {best_xgb_score:.4f}\")\n",
    "\n",
    "# 使用最佳超参数训练最终XGBoost模型\n",
    "best_model = xgb.XGBRegressor(\n",
    "    n_estimators=int(best_xgb_params[0]),\n",
    "    max_depth=int(best_xgb_params[1]),\n",
    "    learning_rate=best_xgb_params[2],\n",
    "    subsample=best_xgb_params[3],\n",
    "    colsample_bytree=best_xgb_params[4],\n",
    "    reg_alpha=best_xgb_params[5],\n",
    "    reg_lambda=best_xgb_params[6],\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "best_model.fit(X_train, y_train)\n",
    "\n",
    "# 评估最终模型\n",
    "y_pred_train = best_model.predict(X_train)\n",
    "y_pred_test = best_model.predict(X_test)\n",
    "\n",
    "r2_train = r2_score(y_train, y_pred_train)\n",
    "r2_test = r2_score(y_test, y_pred_test)\n",
    "\n",
    "rmse_train = np.sqrt(mean_squared_error(y_train, y_pred_train))\n",
    "rmse_test = np.sqrt(mean_squared_error(y_test, y_pred_test))\n",
    "\n",
    "print(\"=== XGBoost模型性能 ===\")\n",
    "print(f\"训练集 R²: {r2_train:.4f}, RMSE: {rmse_train:.4f}\")\n",
    "print(f\"测试集 R²: {r2_test:.4f}, RMSE: {rmse_test:.4f}\")\n",
    "\n",
    "# --- 使用贝叶斯优化寻找TOC去除率>90%的参数组合 ---\n",
    "print(\"\\n开始贝叶斯优化寻找TOC>90%的参数组合...\")\n",
    "\n",
    "# 初始化Fluent接口\n",
    "fluent_interface = FluentMLDataInterface(\n",
    "    fluent_path=\"C:/Program Files/ANSYS Inc/v242/fluent/bin/win64/fluent.exe\",\n",
    "    workbench_path=\"C:/Program Files/ANSYS Inc/v242/Framework/bin/Win64/runwb2.exe\"\n",
    ")\n",
    "\n",
    "# 定义目标函数 - 运行CFD并返回TOC去除率\n",
    "def run_cfd_and_get_toc(params):\n",
    "    # 将优化变量与固定参数组合\n",
    "    full_params = {\n",
    "        'inlet_flow_rate': params[0],\n",
    "        'reactor_diameter': params[1],\n",
    "        'ozone_section_height': params[2],\n",
    "        'catalytic_section_height': params[3],\n",
    "        'membrane_distance': params[4],\n",
    "        'inlet_diameter': params[5],\n",
    "        'inlet_angle': params[6],\n",
    "        **fixed_params\n",
    "    }\n",
    "    \n",
    "    # 创建唯一案例名称\n",
    "    case_name = f\"case_{int(time.time())}_{hash(str(params)) % 10000}\"\n",
    "    \n",
    "    # 创建并运行CFD案例\n",
    "    case_path = fluent_interface.create_fluent_case(case_name, full_params)\n",
    "    success = fluent_interface.run_fluent_simulation(case_path)\n",
    "    \n",
    "    if success:\n",
    "        # 提取结果并添加到数据集\n",
    "        result = fluent_interface.extract_results(case_path)\n",
    "        fluent_interface.add_to_dataset(result)\n",
    "        toc = result['toc_removal_rate']\n",
    "        print(f\"CFD计算完成: TOC去除率 = {toc:.2f}%\")\n",
    "        return -toc  # 返回负值因为我们希望最大化TOC（但skopt最小化目标）\n",
    "    else:\n",
    "        print(\"CFD计算失败，返回默认低值\")\n",
    "        return 100  # 返回一个高值表示失败（因为skopt最小化目标）\n",
    "\n",
    "# 运行贝叶斯优化\n",
    "n_iterations = 200  # 总共运行200次CFD计算\n",
    "high_toc_points = []  # 存储TOC>90%的点\n",
    "best_value = np.inf  # 因为我们最小化 -TOC，所以初始值设为正无穷\n",
    "convergence_count = 0\n",
    "convergence_threshold = 30  # 连续30次无显著改善则停止\n",
    "improvement_tolerance = 1e-4  # 改善程度小于0.01%则认为无显著改善\n",
    "\n",
    "for i in range(n_iterations):\n",
    "    print(f\"\\n=== 迭代 {i+1}/{n_iterations} ===\")\n",
    "    \n",
    "    # 获取下一个建议点\n",
    "    next_point = optimizer.ask()\n",
    "    print(f\"建议参数: {next_point}\")\n",
    "    \n",
    "    # 运行CFD计算\n",
    "    toc_value = -run_cfd_and_get_toc(next_point)  # 转换为正值\n",
    "    \n",
    "    # 记录高TOC的点\n",
    "    if toc_value > 90:\n",
    "        high_toc_points.append((next_point, toc_value))\n",
    "        print(f\"找到高TOC点! 当前已有 {len(high_toc_points)} 个TOC>90%的点\")\n",
    "    \n",
    "    # 告诉优化器结果\n",
    "    optimizer.tell(next_point, -toc_value)  # 传递负值给优化器\n",
    "    \n",
    "    # 收敛监测\n",
    "    current_best = optimizer.yi.min()\n",
    "    current_best_toc = -current_best\n",
    "    \n",
    "    if i == 0:\n",
    "        improvement = np.inf\n",
    "    else:\n",
    "        improvement = (best_value - current_best) / abs(best_value)\n",
    "    \n",
    "    if improvement < improvement_tolerance:\n",
    "        convergence_count += 1\n",
    "    else:\n",
    "        convergence_count = 0\n",
    "        \n",
    "    best_value = current_best\n",
    "    \n",
    "    print(f\"迭代 {i+1}: 当前最佳TOC去除率 = {current_best_toc:.4f}%\")\n",
    "    \n",
    "    # 检查是否收敛\n",
    "    if convergence_count >= convergence_threshold:\n",
    "        print(f\"优化已在迭代 {i+1} 提前收敛。\")\n",
    "        break\n",
    "    \n",
    "    # 每10次迭代重新训练XGBoost模型\n",
    "    if (i + 1) % 10 == 0 and len(fluent_interface.dataset) > 0:\n",
    "        print(\"重新训练XGBoost模型...\")\n",
    "        \n",
    "        # 合并初始数据和新增数据\n",
    "        combined_data = pd.concat([initial_data, fluent_interface.get_dataset()], ignore_index=True)\n",
    "        \n",
    "        # 重新训练XGBoost模型\n",
    "        X_combined = combined_data.drop('toc_removal_rate', axis=1)\n",
    "        y_combined = combined_data['toc_removal_rate']\n",
    "        \n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X_combined, y_combined, test_size=0.2, random_state=42\n",
    "        )\n",
    "        \n",
    "        best_model.fit(X_train, y_train)\n",
    "        \n",
    "        # 评估更新后的模型\n",
    "        y_pred_test = best_model.predict(X_test)\n",
    "        r2_test = r2_score(y_test, y_pred_test)\n",
    "        rmse_test = np.sqrt(mean_squared_error(y_test, y_pred_test))\n",
    "        \n",
    "        print(f\"更新后模型 - 测试集 R²: {r2_test:.4f}, RMSE: {rmse_test:.4f}\")\n",
    "\n",
    "# --- 分析结果 ---\n",
    "print(\"\\n=== 优化结果分析 ===\")\n",
    "\n",
    "# 获取所有高TOC点的参数\n",
    "if high_toc_points:\n",
    "    high_toc_params = [point[0] for point in high_toc_points]\n",
    "    high_toc_values = [point[1] for point in high_toc_points]\n",
    "    \n",
    "    # 计算每个变量的平均值和范围\n",
    "    param_names = [\n",
    "        'inlet_flow_rate', 'reactor_diameter', 'ozone_section_height',\n",
    "        'catalytic_section_height', 'membrane_distance', 'inlet_diameter', 'inlet_angle'\n",
    "    ]\n",
    "    \n",
    "    high_toc_df = pd.DataFrame(high_toc_params, columns=param_names)\n",
    "    high_toc_df['toc_removal_rate'] = high_toc_values\n",
    "    \n",
    "    # 计算每个参数的平均值和范围\n",
    "    result_summary = []\n",
    "    for param in param_names:\n",
    "        min_val = high_toc_df[param].min()\n",
    "        max_val = high_toc_df[param].max()\n",
    "        mean_val = high_toc_df[param].mean()\n",
    "        result_summary.append({\n",
    "            '参数': param,\n",
    "            '最小值': f\"{min_val:.3f}\",\n",
    "            '最大值': f\"{max_val:.3f}\",\n",
    "            '平均值': f\"{mean_val:.3f}\",\n",
    "            '单位': 'm³/h' if 'flow' in param else 'm' if 'diameter' in param or 'height' in param or 'distance' in param else '°'\n",
    "        })\n",
    "    \n",
    "    result_df = pd.DataFrame(result_summary)\n",
    "    print(\"\\nTOC去除率高于90%时的参数范围:\")\n",
    "    print(result_df.to_string(index=False))\n",
    "    \n",
    "    # 保存结果到CSV\n",
    "    result_df.to_csv(\"high_toc_parameters.csv\", index=False)\n",
    "    print(\"\\n结果已保存到 high_toc_parameters.csv\")\n",
    "else:\n",
    "    print(\"未找到TOC去除率高于90%的参数组合\")\n",
    "\n",
    "# --- 可视化优化过程 ---\n",
    "plt.figure(figsize=(10, 6))\n",
    "plot_convergence(optimizer.res)\n",
    "plt.title(\"贝叶斯优化收敛过程\")\n",
    "plt.savefig(\"optimization_convergence.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# --- 最终模型评估 ---\n",
    "print(\"\\n=== 最终模型性能评估 ===\")\n",
    "\n",
    "# 使用所有数据训练最终模型\n",
    "final_data = pd.concat([initial_data, fluent_interface.get_dataset()], ignore_index=True)\n",
    "X_final = final_data.drop('toc_removal_rate', axis=1)\n",
    "y_final = final_data['toc_removal_rate']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_final, y_final, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "best_model.fit(X_train, y_train)\n",
    "\n",
    "# 评估最终模型\n",
    "y_pred_train = best_model.predict(X_train)\n",
    "y_pred_test = best_model.predict(X_test)\n",
    "\n",
    "r2_train = r2_score(y_train, y_pred_train)\n",
    "r2_test = r2_score(y_test, y_pred_test)\n",
    "\n",
    "rmse_train = np.sqrt(mean_squared_error(y_train, y_pred_train))\n",
    "rmse_test = np.sqrt(mean_squared_error(y_test, y_pred_test))\n",
    "\n",
    "print(\"最终模型性能:\")\n",
    "print(f\"训练集 R²: {r2_train:.4f}, RMSE: {rmse_train:.4f}\")\n",
    "print(f\"测试集 R²: {r2_test:.4f}, RMSE: {rmse_test:.4f}\")\n",
    "\n",
    "# 特征重要性分析\n",
    "feature_importance = best_model.feature_importances_\n",
    "feature_names = X_train.columns\n",
    "importance_df = pd.DataFrame({\n",
    "    'feature': feature_names,\n",
    "    'importance': feature_importance\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.barplot(x='importance', y='feature', data=importance_df)\n",
    "plt.title(\"特征重要性\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"feature_importance.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n特征重要性排序:\")\n",
    "print(importance_df.to_string(index=False))\n",
    "\n",
    "# 保存最终模型\n",
    "import joblib\n",
    "joblib.dump(best_model, 'final_xgb_model.pkl')\n",
    "print(\"\\n最终模型已保存为 final_xgb_model.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
