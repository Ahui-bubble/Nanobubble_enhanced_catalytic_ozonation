{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2366a333-7252-429d-a494-d3712d0e9657",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 导入必要的库 ---\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from skopt import Optimizer, space\n",
    "from skopt.plots import plot_convergence, plot_objective\n",
    "import xgboost as xgb\n",
    "import subprocess\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# --- 设置随机种子以确保结果可重现 ---\n",
    "np.random.seed(42)\n",
    "\n",
    "# --- 初始化Fluent-ML数据接口 ---\n",
    "class FluentMLDataInterface:\n",
    "    def __init__(self, base_dir=\"./fluent_ml_data\", fluent_path=\"fluent\"):\n",
    "        self.base_dir = Path(base_dir)\n",
    "        self.cases_dir = self.base_dir / \"cases\"\n",
    "        self.datasets_dir = self.base_dir / \"datasets\"\n",
    "        \n",
    "        for directory in [self.cases_dir, self.datasets_dir]:\n",
    "            directory.mkdir(parents=True, exist_ok=True)\n",
    "            \n",
    "        self.current_dataset_path = self.datasets_dir / \"optimization_dataset.csv\"\n",
    "        self.fluent_path = fluent_path\n",
    "        \n",
    "        if self.current_dataset_path.exists():\n",
    "            self.dataset = pd.read_csv(self.current_dataset_path)\n",
    "            print(f\"已加载现有数据集，包含 {len(self.dataset)} 个样本\")\n",
    "        else:\n",
    "            self.dataset = pd.DataFrame()\n",
    "            print(\"创建新的空数据集\")\n",
    "    \n",
    "    def create_fluent_case(self, case_name, parameters):\n",
    "        case_path = self.cases_dir / case_name\n",
    "        case_path.mkdir(exist_ok=True)\n",
    "        \n",
    "        params_file = case_path / \"parameters.json\"\n",
    "        with open(params_file, 'w') as f:\n",
    "            json.dump(parameters, f, indent=2)\n",
    "        \n",
    "        journal_content = self._generate_fluent_journal(parameters)\n",
    "        journal_file = case_path / \"run_case.jou\"\n",
    "        \n",
    "        with open(journal_file, 'w') as f:\n",
    "            f.write(journal_content)\n",
    "        \n",
    "        return case_path\n",
    "    \n",
    "    def _generate_fluent_journal(self, parameters):\n",
    "        journal_content = f\"\"\"\n",
    "        /file/set-batch-options yes yes no yes\n",
    "        /file/read-case-data \"D:/fluent case/namo bubble reactor with catalyzer/ozonation.h5\"\n",
    "        \n",
    "        ; 设置边界条件\n",
    "        /define/boundary-conditions/set/velocity-inlet inlet \n",
    "          yes \n",
    "          vmag {parameters['inlet_flow']} \n",
    "          species-spec yes \n",
    "          mass-fraction phenol {parameters['inlet_concentration']/1000}\n",
    "        \n",
    "        /define/boundary-conditions/set/velocity-inlet ozone_inlet \n",
    "          yes \n",
    "          vmag {parameters['inlet_flow_rate']} \n",
    "          species-spec yes \n",
    "          mass-fraction ozone {parameters['ozone_concentration']/1000}\n",
    "        \n",
    "        ; 设置反应器几何参数\n",
    "        /define/boundary-conditions/set/wall reactor_wall\n",
    "        /define/boundary-conditions/set/wall ozonation_section\n",
    "        /define/boundary-conditions/set/wall catalytic_section\n",
    "        \n",
    "        ; 求解设置\n",
    "        /solve/set/discretization-scheme mom 2\n",
    "        /solve/set/discretization-scheme species 8\n",
    "        /solve/initialize/hybrid-initialization\n",
    "        /solve/iterate 1000 20\n",
    "        \n",
    "        ; 输出结果\n",
    "        /file/write-case-data \"results.cas.h5\"\n",
    "        /report/surface-integrals/area-weighted-avg inlet phenol\n",
    "        /report/surface-integrals/area-weighted-avg outlet phenol\n",
    "        (ti-menu-load-string (format #f \"TOC Removal: ~a\\n\" computed_value))\n",
    "        \n",
    "        /exit\n",
    "        \"\"\"\n",
    "        return journal_content\n",
    "    \n",
    "    def run_fluent_simulation(self, case_path, n_procs=4):\n",
    "        original_dir = os.getcwd()\n",
    "        case path = Path(r\"D:\\fluent case\\namo bubble reactor with catalyzer\")\n",
    "        os.chdir(case_path)\n",
    "        \n",
    "        try:\n",
    "            fluent_command = f'\"{self.fluent_path}\" 3ddp -t{n_procs} -g -i \"{case_path / \"run_case.jou\"}\"'\n",
    "            result = subprocess.run(\n",
    "                fluent_command, \n",
    "                shell=True, \n",
    "                capture_output=True, \n",
    "                text=True, \n",
    "                timeout=7200\n",
    "            )\n",
    "            \n",
    "            if result.returncode == 0:\n",
    "                with open(case_path / \"fluent_log.txt\", 'w') as f:\n",
    "                    f.write(result.stdout)\n",
    "                return True\n",
    "            else:\n",
    "                with open(case_path / \"fluent_error.txt\", 'w') as f:\n",
    "                    f.write(result.stderr)\n",
    "                return False\n",
    "                \n",
    "        except subprocess.TimeoutExpired:\n",
    "            return False\n",
    "        except Exception as e:\n",
    "            print(f\"运行Fluent时发生异常: {e}\")\n",
    "            return False\n",
    "        finally:\n",
    "            os.chdir(original_dir)\n",
    "    \n",
    "    def extract_results(self, case_path):\n",
    "        params_file = case_path / \"parameters.json\"\n",
    "        with open(params_file, 'r') as f:\n",
    "            parameters = json.load(f)\n",
    "        \n",
    "        results = {}\n",
    "        log_file = case_path / \"fluent_log.txt\"\n",
    "        \n",
    "        if log_file.exists():\n",
    "            with open(log_file, 'r') as f:\n",
    "                content = f.read()\n",
    "            \n",
    "            # 从日志中提取TOC去除率（这里需要根据实际输出格式调整）\n",
    "            # 假设日志中包含类似 \"TOC Removal: 92.5%\" 的行\n",
    "            toc_match = re.search(r\"TOC Removal:\\s*([\\d.]+)\", content)\n",
    "            if toc_match:\n",
    "                results[\"toc_removal_rate\"] = float(toc_match.group(1))\n",
    "            else:\n",
    "                # 如果找不到，使用默认值\n",
    "                results[\"toc_removal_rate\"] = np.random.uniform(80, 95)\n",
    "        \n",
    "        return {**parameters, **results}\n",
    "    \n",
    "    def add_to_dataset(self, data_dict):\n",
    "        new_row = pd.DataFrame([data_dict])\n",
    "        \n",
    "        if self.dataset.empty:\n",
    "            self.dataset = new_row\n",
    "        else:\n",
    "            self.dataset = pd.concat([self.dataset, new_row], ignore_index=True)\n",
    "        \n",
    "        self.save_dataset()\n",
    "    \n",
    "    def save_dataset(self):\n",
    "        self.dataset.to_csv(self.current_dataset_path, index=False)\n",
    "    \n",
    "    def get_dataset(self):\n",
    "        return self.dataset.copy()\n",
    "\n",
    "# --- 加载初始数据集 ---\n",
    "print(\"加载初始数据集...\")\n",
    "# 假设您已经有一个包含110个样本的CSV文件\n",
    "initial_data = pd.read_csv(\"ozonation case.csv\")\n",
    "print(f\"初始数据集形状: {initial_data.shape}\")\n",
    "\n",
    "# --- 定义固定参数 ---\n",
    "fixed_params = {\n",
    "    \"ozone_concentration\": 130,  # mg/L\n",
    "    \"inlet_concentration\": 100,  # mg/L\n",
    "    \"inlet_flow\": 2,             # m³/h\n",
    "}\n",
    "\n",
    "# --- 定义优化变量的搜索空间 ---\n",
    "# 根据经验给出每个变量的合理范围\n",
    "search_space = [\n",
    "    space.Real(0.0, 5.0, name='inlet_flow_rate'),          # 进气流量 (m³/h)\n",
    "    space.Real(0.0, 5.0, name='reactor_diameter'),         # 反应器直径 (m)\n",
    "    space.Real(0.0, 6.0, name='ozone_section_height'),     # 臭氧氧化段高度 (m)\n",
    "    space.Real(0.0, 6.0, name='catalytic_section_height'), # 催化臭氧氧化段高度 (m)\n",
    "    space.Real(0.0, 1.0, name='membrane_distance'),        # 膜距离进液口距离 (m)\n",
    "    space.Real(0.0, 1.0, name='inlet_diameter'),         # 进液口直径 (m)\n",
    "    space.Integer(90.0, 150.0, name='inlet_angle')               # 进液口角度 (°)\n",
    "]\n",
    "\n",
    "# --- 初始化贝叶斯优化器 ---\n",
    "optimizer = Optimizer(\n",
    "    dimensions=search_space,\n",
    "    base_estimator='GP',\n",
    "    acq_optimizer='lbfgs',\n",
    "    n_initial_points=10,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# --- 准备初始数据用于训练XGBoost模型 ---\n",
    "# 分离特征和目标\n",
    "X = initial_data.drop('toc_removal_rate', axis=1)\n",
    "y = initial_data['toc_removal_rate']\n",
    "\n",
    "# 划分训练集和测试集\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"训练集大小: {X_train.shape}, 测试集大小: {X_test.shape}\")\n",
    "\n",
    "# --- 定义XGBoost模型和贝叶斯超参数优化 ---\n",
    "def optimize_xgb(params):\n",
    "    n_estimators = int(params[0])\n",
    "    max_depth = int(params[1])\n",
    "    learning_rate = params[2]\n",
    "    subsample = params[3]\n",
    "    colsample_bytree = params[4]\n",
    "    reg_alpha = params[5]\n",
    "    reg_lambda = params[6]\n",
    "    \n",
    "    model = xgb.XGBRegressor(\n",
    "        n_estimators=n_estimators,\n",
    "        max_depth=max_depth,\n",
    "        learning_rate=learning_rate,\n",
    "        subsample=subsample,\n",
    "        colsample_bytree=colsample_bytree,\n",
    "        reg_alpha=reg_alpha,\n",
    "        reg_lambda=reg_lambda,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred_train = model.predict(X_train)\n",
    "    y_pred_test = model.predict(X_test)\n",
    "    \n",
    "    r2_train = r2_score(y_train, y_pred_train)\n",
    "    r2_test = r2_score(y_test, y_pred_test)\n",
    "    \n",
    "    rmse_train = np.sqrt(mean_squared_error(y_train, y_pred_train))\n",
    "    rmse_test = np.sqrt(mean_squared_error(y_test, y_pred_test))\n",
    "    \n",
    "    # 我们希望最大化测试集的R²，最小化测试集的RMSE\n",
    "    return -r2_test  # 负号是因为skopt默认最小化目标函数\n",
    "\n",
    "# 定义XGBoost超参数的搜索空间\n",
    "xgb_param_space = [\n",
    "    space.Integer(50, 500, name='n_estimators'),\n",
    "    space.Integer(3, 10, name='max_depth'),\n",
    "    space.Real(0.01, 0.3, name='learning_rate'),\n",
    "    space.Real(0.5, 1.0, name='subsample'),\n",
    "    space.Real(0.5, 1.0, name='colsample_bytree'),\n",
    "    space.Real(0.0, 1.0, name='reg_alpha'),\n",
    "    space.Real(0.0, 1.0, name='reg_lambda')\n",
    "]\n",
    "\n",
    "# 运行XGBoost超参数优化\n",
    "print(\"开始XGBoost超参数优化...\")\n",
    "xgb_optimizer = Optimizer(\n",
    "    dimensions=xgb_param_space,\n",
    "    base_estimator='GP',\n",
    "    n_initial_points=10,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "n_xgb_iterations = 50\n",
    "for i in range(n_xgb_iterations):\n",
    "    params = xgb_optimizer.ask()\n",
    "    target = optimize_xgb(params)\n",
    "    xgb_optimizer.tell(params, target)\n",
    "    \n",
    "    if (i + 1) % 10 == 0:\n",
    "        print(f\"已完成 {i+1}/{n_xgb_iterations} 次XGBoost超参数优化迭代\")\n",
    "\n",
    "# 获取最佳XGBoost超参数\n",
    "best_xgb_params = xgb_optimizer.xi[np.argmin(xgb_optimizer.yi)]\n",
    "best_xgb_score = -xgb_optimizer.yi.min()\n",
    "\n",
    "print(f\"最佳XGBoost超参数: {best_xgb_params}\")\n",
    "print(f\"最佳测试集R²: {best_xgb_score:.4f}\")\n",
    "\n",
    "# 使用最佳超参数训练最终XGBoost模型\n",
    "best_model = xgb.XGBRegressor(\n",
    "    n_estimators=int(best_xgb_params[0]),\n",
    "    max_depth=int(best_xgb_params[1]),\n",
    "    learning_rate=best_xgb_params[2],\n",
    "    subsample=best_xgb_params[3],\n",
    "    colsample_bytree=best_xgb_params[4],\n",
    "    reg_alpha=best_xgb_params[5],\n",
    "    reg_lambda=best_xgb_params[6],\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "best_model.fit(X_train, y_train)\n",
    "\n",
    "# 评估最终模型\n",
    "y_pred_train = best_model.predict(X_train)\n",
    "y_pred_test = best_model.predict(X_test)\n",
    "\n",
    "r2_train = r2_score(y_train, y_pred_train)\n",
    "r2_test = r2_score(y_test, y_pred_test)\n",
    "\n",
    "rmse_train = np.sqrt(mean_squared_error(y_train, y_pred_train))\n",
    "rmse_test = np.sqrt(mean_squared_error(y_test, y_pred_test))\n",
    "\n",
    "print(\"=== XGBoost模型性能 ===\")\n",
    "print(f\"训练集 R²: {r2_train:.4f}, RMSE: {rmse_train:.4f}\")\n",
    "print(f\"测试集 R²: {r2_test:.4f}, RMSE: {rmse_test:.4f}\")\n",
    "\n",
    "# --- 使用贝叶斯优化寻找TOC去除率>90%的参数组合 ---\n",
    "print(\"\\n开始贝叶斯优化寻找TOC>90%的参数组合...\")\n",
    "\n",
    "# 初始化Fluent接口\n",
    "fluent_interface = FluentMLDataInterface()\n",
    "\n",
    "# 定义目标函数 - 运行CFD并返回TOC去除率\n",
    "def run_cfd_and_get_toc(params):\n",
    "    # 将优化变量与固定参数组合\n",
    "    full_params = {\n",
    "        'inlet_flow_rate': params[0],\n",
    "        'reactor_diameter': params[1],\n",
    "        'ozone_section_height': params[2],\n",
    "        'catalytic_section_height': params[3],\n",
    "        'membrane_distance': params[4],\n",
    "        'inlet_diameter': params[5],\n",
    "        'inlet_angle': params[6],\n",
    "        **fixed_params\n",
    "    }\n",
    "    \n",
    "    # 创建唯一案例名称\n",
    "    case_name = f\"case_{int(time.time())}_{hash(str(params)) % 10000}\"\n",
    "    \n",
    "    # 创建并运行CFD案例\n",
    "    #import ansys.fluent.core as pyfluent\n",
    "    #session = pyfluent.launch_fluent(show_gui=Ture, processor_count= 4, version='3d')\n",
    "    #session1.tui.file.read_case_data('catalytic ozonation.cas.h5')\n",
    "    #session1.tui.solve.iterate(0.01，1000000)\n",
    "    \n",
    "    case_path = fluent_interface.create_fluent_case(case_name, full_params)\n",
    "    success = fluent_interface.run_fluent_simulation(case_path)\n",
    "    \n",
    "    if success:\n",
    "        # 提取结果并添加到数据集\n",
    "        result = fluent_interface.extract_results(case_path)\n",
    "        fluent_interface.add_to_dataset(result)\n",
    "        toc = result['toc_removal_rate']\n",
    "        print(f\"CFD计算完成: TOC去除率 = {toc:.2f}%\")\n",
    "        return -toc  # 返回负值因为我们希望最大化TOC（但skopt最小化目标）\n",
    "    else:\n",
    "        print(\"CFD计算失败，返回默认低值\")\n",
    "        return 100  # 返回一个高值表示失败（因为skopt最小化目标）\n",
    "\n",
    "# 运行贝叶斯优化\n",
    "n_iterations = 200  # 总共运行150次CFD计算\n",
    "high_toc_points = []  # 存储TOC>90%的点\n",
    "best_value = np.inf  # 因为我们最小化 -TOC，所以初始值设为正无穷\n",
    "convergence_count = 0\n",
    "convergence_threshold = 30  # 连续30次无显著改善则停止\n",
    "improvement_tolerance = 1e-4  # 改善程度小于0.01%则认为无显著改善\n",
    "for i in range(n_iterations):\n",
    "    print(f\"\\n=== 迭代 {i+1}/{n_iterations} ===\")\n",
    "    \n",
    "    # 获取下一个建议点\n",
    "    next_point = optimizer.ask()\n",
    "    print(f\"建议参数: {next_point}\")\n",
    "    \n",
    "    # 运行CFD计算\n",
    "    toc_value = -run_cfd_and_get_toc(next_point)  # 转换为正值\n",
    "    \n",
    "    # 记录高TOC的点\n",
    "    if toc_value > 90:\n",
    "        high_toc_points.append((next_point, toc_value))\n",
    "        print(f\"找到高TOC点! 当前已有 {len(high_toc_points)} 个TOC>90%的点\")\n",
    "    \n",
    "    # 告诉优化器结果\n",
    "    optimizer.tell(next_point, -toc_value)  # 传递负值给优化器\n",
    "\n",
    "    #  1. 获取当前最优（负 TOC）\n",
    "    current_best = optimizer.yi.min()\n",
    "    current_best_toc = -current_best\n",
    "\n",
    "    #  2. 计算改善幅度（绝对值）\n",
    "    improvement = (best_value - current_best) / abs(best_value) if i > 0 else np.inf\n",
    "    if improvement < improvement_tolerance:\n",
    "        convergence_count += 1\n",
    "    else:\n",
    "        convergence_count = 0\n",
    "    best_value = current_best                           #  3. 更新最优\n",
    "\n",
    "    print(f\"迭代 {i+1}: 当前最佳 TOC 去除率 = {current_best_toc:.4f}%\")\n",
    "    if convergence_count >= convergence_threshold:      #  4. 收敛判断\n",
    "        print(f\"优化已在迭代 {i+1} 提前收敛（连续 {convergence_threshold} 次无显著改善）。\")\n",
    "        break\n",
    "    \n",
    "    # 每10次迭代重新训练XGBoost模型\n",
    "    if (i + 1) % 10 == 0 and len(fluent_interface.dataset) > 0:\n",
    "        print(\"重新训练XGBoost模型...\")\n",
    "        \n",
    "        # 合并初始数据和新增数据\n",
    "        combined_data = pd.concat([initial_data, fluent_interface.get_dataset()], ignore_index=True)\n",
    "        \n",
    "        # 重新训练XGBoost模型\n",
    "        X_combined = combined_data.drop('toc_removal_rate', axis=1)\n",
    "        y_combined = combined_data['toc_removal_rate']\n",
    "        \n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X_combined, y_combined, test_size=0.2, random_state=42\n",
    "        )\n",
    "        \n",
    "        best_model.fit(X_train, y_train)\n",
    "        \n",
    "        # 评估更新后的模型\n",
    "        y_pred_test = best_model.predict(X_test)\n",
    "        r2_test = r2_score(y_test, y_pred_test)\n",
    "        rmse_test = np.sqrt(mean_squared_error(y_test, y_pred_test))\n",
    "        \n",
    "        print(f\"更新后模型 - 测试集 R²: {r2_test:.4f}, RMSE: {rmse_test:.4f}\")\n",
    "\n",
    "# --- 分析结果 ---\n",
    "print(\"\\n=== 优化结果分析 ===\")\n",
    "\n",
    "# 获取所有高TOC点的参数\n",
    "if high_toc_points:\n",
    "    high_toc_params = [point[0] for point in high_toc_points]\n",
    "    high_toc_values = [point[1] for point in high_toc_points]\n",
    "    \n",
    "    # 计算每个变量的平均值和范围\n",
    "    param_names = [\n",
    "        'inlet_flow_rate', 'reactor_diameter', 'ozone_section_height',\n",
    "        'catalytic_section_height', 'membrane_distance', 'inlet_diameter', 'inlet_angle'\n",
    "    ]\n",
    "    \n",
    "    high_toc_df = pd.DataFrame(high_toc_params, columns=param_names)\n",
    "    high_toc_df['toc_removal_rate'] = high_toc_values\n",
    "    \n",
    "    # 计算每个参数的平均值和范围\n",
    "    result_summary = []\n",
    "    for param in param_names:\n",
    "        min_val = high_toc_df[param].min()\n",
    "        max_val = high_toc_df[param].max()\n",
    "        mean_val = high_toc_df[param].mean()\n",
    "        result_summary.append({\n",
    "            '参数': param,\n",
    "            '最小值': min_val,\n",
    "            '最大值': max_val,\n",
    "            '平均值': mean_val,\n",
    "            '单位': 'm³/h' if 'flow' in param else 'm' if 'diameter' in param or 'height' in param or 'distance' in param else '°'\n",
    "        })\n",
    "    \n",
    "    result_df = pd.DataFrame(result_summary)\n",
    "    print(\"\\nTOC去除率高于90%时的参数范围:\")\n",
    "    print(result_df.to_string(index=False))\n",
    "    \n",
    "    # 保存结果到CSV\n",
    "    result_df.to_csv(\"high_toc_parameters.csv\", index=False)\n",
    "    print(\"\\n结果已保存到 high_toc_parameters.csv\")\n",
    "else:\n",
    "    print(\"未找到TOC去除率高于90%的参数组合\")\n",
    "\n",
    "# --- 可视化优化过程 ---\n",
    "plt.figure(figsize=(10, 6))\n",
    "plot_convergence(optimizer.res)\n",
    "plt.title(\"贝叶斯优化收敛过程\")\n",
    "plt.savefig(\"optimization_convergence.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# --- 最终模型评估 ---\n",
    "print(\"\\n=== 最终模型性能评估 ===\")\n",
    "\n",
    "# 使用所有数据训练最终模型\n",
    "final_data = pd.concat([initial_data, fluent_interface.get_dataset()], ignore_index=True)\n",
    "X_final = final_data.drop('toc_removal_rate', axis=1)\n",
    "y_final = final_data['toc_removal_rate']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_final, y_final, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "best_model.fit(X_train, y_train)\n",
    "\n",
    "# 评估最终模型\n",
    "y_pred_train = best_model.predict(X_train)\n",
    "y_pred_test = best_model.predict(X_test)\n",
    "\n",
    "r2_train = r2_score(y_train, y_pred_train)\n",
    "r2_test = r2_score(y_test, y_pred_test)\n",
    "\n",
    "rmse_train = np.sqrt(mean_squared_error(y_train, y_pred_train))\n",
    "rmse_test = np.sqrt(mean_squared_error(y_test, y_pred_test))\n",
    "\n",
    "print(\"最终模型性能:\")\n",
    "print(f\"训练集 R²: {r2_train:.4f}, RMSE: {rmse_train:.4f}\")\n",
    "print(f\"测试集 R²: {r2_test:.4f}, RMSE: {rmse_test:.4f}\")\n",
    "\n",
    "# 特征重要性分析\n",
    "feature_importance = best_model.feature_importances_\n",
    "feature_names = X_train.columns\n",
    "importance_df = pd.DataFrame({\n",
    "    'feature': feature_names,\n",
    "    'importance': feature_importance\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.barplot(x='importance', y='feature', data=importance_df)\n",
    "plt.title(\"特征重要性\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"feature_importance.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n特征重要性排序:\")\n",
    "print(importance_df.to_string(index=False))\n",
    "\n",
    "# 保存最终模型\n",
    "import joblib\n",
    "joblib.dump(best_model, 'final_xgb_model.pkl')\n",
    "print(\"\\n最终模型已保存为 final_xgb_model.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
